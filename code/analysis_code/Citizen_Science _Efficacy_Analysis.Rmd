---
title: "Citizen Science Efficacy Analysis"
author: "William Norfolk"
date: "11/6/2019"
output: word_document
---

This script measures the efficacy of citizen science data gathered in this study by investigating the individual water quality parameters that characterize the seperation of observations into the selected outcome variable island_side. Selection of island side will be evaluated using tree-based modeling to detemrine which water quality parameters strongly influence the the seperation of water systems. The goal of this analysis is not to determine which parameters are the most powerful for influencing data seperation, but rather to measure the most powerful variables and compare them to known conditions of water quality seperation. If the tree analysis of our citizen science data produces similar keystone predictor variables to our known strong predictors of water quality this will suggest that the citizen science data used is consistent with the actual condition of Key Largo waters thus showing providing a measure of efficacy for the usage of this citizen science data.

## Model Prep

Start by loading the libraries.
```{r add libs}
library('tidyr')
library('dplyr')
library('forcats')
library('ggplot2')
library('knitr')
library('caret')
library('doParallel')
library('rpart')
library('rpart.plot')
library('mda')
library('ranger')
library('e1071')
library('visdat')
library('filesstrings')
```
Load the data and take a look.

```{r load and glimpse}
WQ_clean_data <- readRDS("../../data/processed_data/processeddata.rds")

glimpse(WQ_clean_data)
```

Check for NAs and properly formatted classes.

```{r visdat}
vis_dat(WQ_clean_data)
```
Data looks pretty good, there are some NA values but the tree models should be able to handel these since they are not strongly grouped. 

We do need to recode a few variables in this view. We will make all or date related variables numeric, and the remaining buckets will be factors. Also we can remove instructor_name and kit since they are not very relevent to the data. For now we will leave group_name in the data in case different schools are more effective measurers of water quality. 



```{r modify and vis}
#Drop instructor name and equipment
reduce_vars <- WQ_clean_data %>% dplyr::select(-instructor_name, -equipment)

#Change numerics
reduce_vars$Month <- as.numeric(as.character(reduce_vars$Month))
reduce_vars$Day <- as.numeric(as.character(reduce_vars$Day))
reduce_vars$Year <- as.numeric(as.character(reduce_vars$Year))

#Change factors
reduce_vars$group_name <- as.factor(as.character(reduce_vars$group_name))
reduce_vars$island_side <- as.factor(as.character(reduce_vars$island_side))
reduce_vars$site_type <- as.factor(as.character(reduce_vars$site_type))
reduce_vars$location <- as.factor(as.character(reduce_vars$location))


vis_dat(reduce_vars)
```

This data looks ready to use. We may make futher modifications downstream as models produce results.

## Data spllitting

Before we split we need to ensure the NAs are removed from the outcome and the outcome is in the first column of the dataframe.

```{r drop NA outomes and reorder}
model_ready <- reduce_vars[!is.na(reduce_vars$island_side), ]

model_ready <- model_ready[, c(12,1,2,3,4,5,6,7,8,9,10,11,13)]

```


We will split our data for K-fold cross-validation into a 70/30 train and test set.


```{r data split}

set.seed(123)

trainset <- caret::createDataPartition(y = model_ready$island_side, p = 0.7, list = FALSE)

#Extract observations/rows for training and assign to new variable
data_train = model_ready[trainset,] 

#Same as above but for the test set
data_test = model_ready[-trainset,] 
```



```{r}
data_train <- na.omit(data_train)
data_test <- na.omit(data_test)
```


## Null Model

First we need to determine our null model. Since our outcome is a categorical predictor, a null model should always predict the most common category. Four our dataset the oceanside is the most frequently visited island_side. We will use accuracy to measure how well our model predicts the island_side given the five water quality parameters.

```{r}

library(mlr) #We will jump between mlr and caret so call with :: at all times. 

mlr::measureACC("ocean", model_ready$island_side)

```

It looks like we have an accuracy of about 0.62 for our null lets see if we can do better. 


Now lets see if we can do better with some models. We will start with single a single predictor model to determine accuracy for each predictor variable.

```{r single-predictor tree acc}

set.seed(1111) #To keep reproducibility

outcomename = "island_side" #Name of outcome of interest

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #Sets K-fold cross-validation.

Npred <- ncol(data_train)-1 #Sets predictors minus the outcome in column 1.

resultmat <- data.frame(Variable = names(data_train)[-1], Accuracy = rep(0,Npred)) #Takes results and moves to dataframe named resultmat, measure set to accuracy.

for (n in 2:ncol(data_train))
{
  fit1 <- caret::train( as.formula(paste(outcomename, "~",names(data_train)[n])) , data = data_train, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 10) 
resultmat[n-1,2]= max(fit1$results$Accuracy)  
} #Loop to all variables through prediction. Method rpart = tree algorithm, and skipping any NA values. 

print(resultmat)

```

So the single-predictor looks pretty interesting. The majority of the predictors are around the same level as the null, however we do have some spikes. Location and site_type have a very high accuracy. This is likely due to the fact that they are directly associated with the island_side. We will likely have to remove these down the line since they are too powerful. Salinity is a bit stronger than the null as well, this is likely due to the large split in salinity from each island side we saw in the visualization. This is likely to become a very strong predictor in the model, and there is a possibility that salinity alone is enough to establish island_side. 

We will continue with a full model.

```{r full-predictor tree acc}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit1 = caret::train(island_side  ~ ., data=data_train, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 10) 

print(fit1$results)

```
 Looks like we have pretty high accuracy across the board here. This is likely due to the two predictors mentioned above. Lets take a look at the tree.

```{r}
prp(fit1$finalModel, extra = 106, type = 2)
```

As expected our tree splits along the lines of site_type and location. This makes complete sense since island_side and site_type and essentially large subgroups of the various location information.To get a comprehensive picture of water qiality-based predictions we will remove these two variables from our data set. 

```{r reduce-further}

data_train <- data_train %>% dplyr::select(-location, -site_type)
data_test <- data_test %>% dplyr::select(-location, -site_type)

```

Lets try the models again
```{r single pred round two}
set.seed(1111) 
outcome = "island_side"
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 
Npred <- ncol(data_train)-1
resultmat <- data.frame(Variable = names(data_train)[-1], Accuracy = rep(0,Npred))
for (n in 2:ncol(data_train))
{
  fit_single_reduced <- caret::train( as.formula(paste(outcomename, "~",names(data_train)[n])) , data = data_train, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 20) 
resultmat[n-1,2]= max(fit_single_reduced$results$Accuracy)  
}

print(resultmat)

prp(fit_single_reduced$finalModel, extra = 106, type = 2)
```
It looks like salinity is the strongest water quality predictor of island side in the dataset. Lets see if the full fit changes anything. 

```{r multipredictor fit}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit2 = caret::train(island_side  ~ ., data=data_train, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 20) 

print(fit2$results)

prp(fit2$finalModel, extra = 106, type = 2)

```

The resulting tree (while a bit boring looking) is actually quite interesting. This suggests that salinity alone may be the best predictor of island_side. While relatively simple, this prediction is consistent with the known seperation of water quality data from bayside and oceanside waters and is thus not suprisingly the strongest factor for island_side. 


Now that we know salinity is the most important factor for determining island_side lets delve into the other predictors with salinity removed. 

```{r drop sal}
data_train_no_sal <- data_train %>% dplyr::select(-salinity)
data_test_no_sal <- data_test %>% dplyr::select(-salinity)

```

```{r no sal single fit}
set.seed(1111) 
outcome = "island_side"
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 
Npred <- ncol(data_train_no_sal)-1
resultmat <- data.frame(Variable = names(data_train_no_sal)[-1], Accuracy = rep(0,Npred))
for (n in 2:ncol(data_train_no_sal))
{
  fit_no_sal <- caret::train( as.formula(paste(outcomename, "~",names(data_train_no_sal)[n])) , data = data_train, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 20) 
resultmat[n-1,2]= max(fit_no_sal$results$Accuracy)  
}

print(resultmat)

prp(fit_no_sal$finalModel, extra = 106, type = 2)

```
Interesting, it looks like in the absence of salinity water temperature is the second strongest predictor of island side. Water splits into two distinct groups in this tree. First water teperature greater than or equal to 31 degrees is associated with the bayside. Additionally water less than 20 degrees is also associated with the bayside. This is consistent with known temperature fluctuations and our violin plots which suggests that bayside waters are eurythermal. Lets see if the full fit changes anything.

```{r no sal full fit}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit_no_sal_full = caret::train(island_side  ~ ., data=data_train_no_sal, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 20) 

print(fit_no_sal_full$results)

prp(fit_no_sal_full$finalModel, extra = 106, type = 2)

```

The full fit drops the second temperature branch, this is likely due to a low number of observations below 20 degrees, which is extremely cold for Key Largo however not unreasonable. 

## Model Testing

Lastly, lets evaluate our test data with the same models and generate confusion matricies. 

```{r test single}
set.seed(1111) 
outcome = "island_side"
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 
Npred <- ncol(data_test)-1
resultmat <- data.frame(Variable = names(data_test)[-1], Accuracy = rep(0,Npred))
for (n in 2:ncol(data_test))
{
  fit_test_single <- caret::train( as.formula(paste(outcomename, "~",names(data_test)[n])) , data = data_test, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 20) 
resultmat[n-1,2]= max(fit_test_single$results$Accuracy)  
}

print(resultmat)

prp(fit_test_single$finalModel, extra = 106, type = 2)

```
Single predictor assessment is pretty similar, however it seperated the data at 33 ppt salinity.

```{r test multi}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit_test_multi = caret::train(island_side  ~ ., data=data_test, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 20) 

print(fit_test_multi$results)

prp(fit_test_multi$finalModel, extra = 106, type = 2)
```

Multi predictor seperates at 33 ppt as well with a small change in the percentages. Regardless we can confidantly state that salinity is the strongest measure of water quality to determine island side. 

And now to remove salinity.

```{r}
set.seed(1111) 
outcome = "island_side"
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 
Npred <- ncol(data_test_no_sal)-1
resultmat <- data.frame(Variable = names(data_test_no_sal)[-1], Accuracy = rep(0,Npred))
for (n in 2:ncol(data_test_no_sal))
{
  fit_test_single_no_sal <- caret::train( as.formula(paste(outcomename, "~",names(data_test_no_sal)[n])) , data = data_test_no_sal, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 20) 
resultmat[n-1,2]= max(fit_test_single_no_sal$results$Accuracy)  
}

print(resultmat)

prp(fit_test_single_no_sal$finalModel, extra = 106, type = 2)
```

Test data seperates water only by the lowest water temperature at 22 degrees.

```{r}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit_test_multi_no_sal = caret::train(island_side  ~ ., data=data_test_no_sal, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 20) 

print(fit_test_multi_no_sal$results)

prp(fit_test_multi_no_sal$finalModel, extra = 106, type = 2)
```
Humm interesting in the absence of salinity, dissolved oxygen is now the dominat predictor in the test data. 

All very interesting analyses, but after all of that it looks like a simple measure of salinity (fit2) is the best method to determine the isalnd side of a sample. This is consistent with our prior data visualizations and with known characteristics of esturarine and marine waters.

Lets try a prediction.

```{r confusion matrix}
train_predict <- predict(fit2, data_train)
confusionMatrix(train_predict, data_train$island_side)$table

test_predict <- predict(fit2, data_test)
confusionMatrix(test_predict, data_test$island_side)$table

```

It looks like our model is reasonably effective at identifying ocean side sites, however a bit worse when trying to classify bayside sites. 


Perhaps a predictor of site type may be more appropriate for this study.

```{r}
glimpse(model_ready)
```

```{r outcome 2}
outcome_2 <- model_ready %>% dplyr::select(-location, -island_side)

outcome_2 <- na.omit(outcome_2)

```

```{r}
outcome_2 <- outcome_2[, c(11,1,2,3,4,5,6,7,8,9,10)]
```

```{r data split 2}

set.seed(123)

trainset <- caret::createDataPartition(y = outcome_2$site_type, p = 0.7, list = FALSE)

#Extract observations/rows for training and assign to new variable
data_train_2 = outcome_2[trainset,] 

#Same as above but for the test set
data_test_2 = outcome_2[-trainset,] 
```


```{r null model 2}

library(mlr) #We will jump between mlr and caret so call with :: at all times. 

mlr::measureACC("Seagrass/Mangrove", outcome_2$site_type)


```

```{r single pred outcome 2}

set.seed(1111) #To keep reproducibility

outcomename = "site_type" #Name of outcome of interest

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #Sets K-fold cross-validation.

Npred <- ncol(data_train)-1 #Sets predictors minus the outcome in column 1.

resultmat <- data.frame(Variable = names(data_train_2)[-1], Accuracy = rep(0,Npred)) #Takes results and moves to dataframe named resultmat, measure set to accuracy.

for (n in 2:ncol(data_train_2))
{
  fit11 <- caret::train( as.formula(paste(outcomename, "~",names(data_train_2)[n])) , data = data_train_2, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 10) 
resultmat[n-1,2]= max(fit11$results$Accuracy)  
} #Loop to all variables through prediction. Method rpart = tree algorithm, and skipping any NA values. 

print(resultmat)

prp(fit11$finalModel, extra = 8, type = 2)

```
```{r multi pred outcome 2}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit111 = caret::train(site_type  ~ ., data=data_train_2, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 10) 

print(fit111$results)

prp(fit111$finalModel, extra = 8, type = 2)

```
Swapping outcomes it appears that salinity remains the intial deciding variable. Data is either grouped into Seagrass/Mangrove or not. This grouping makes sense because all bayside sites are Seagrass/Mangrove, however oceanside can be a mix of all three. Coral Reef and Patch Reef/Hardbottom seperates by water temperature primairly which is a reasonable branch as patch reefs tend to be cooler in temperature than coral reefs. Beyond these branches there are a few branches based off of date/year data. While this clustering is interesting to note, it does not clarify much about our overall goal of measuring the efficacy of citizen science data as sampling time/date is not under the control of the citizen scientists. This analysis and the trees above suggest, it would be most prudent to build our trees entirely from the variables that the citizen scientists can control as it would not be appropriate to measure their efficacy with items outside of their control. 

## Final Model Fitting Evaluation

We will measure both outcomes island_side and site_type with datasets which only contain variables controlled by the citizen scientists. 

```{r CS only island side}

cs_only_side <- model_ready %>% dplyr::select(-Month, -Day, -Year, -military_time, -location, -group_name, -site_type)

cs_only_side <- na.omit(cs_only_side)

glimpse(cs_only_side)
```

```{r split data 3}
set.seed(123)

trainset <- caret::createDataPartition(y = cs_only_side$island_side, p = 0.7, list = FALSE)

#Extract observations/rows for training and assign to new variable
data_train_cs_1 = cs_only_side[trainset,] 

#Same as above but for the test set
data_test_cs_2 = cs_only_side[-trainset,] 
```

```{r null}
mlr::measureACC("ocean", cs_only_side$island_side)

```

```{r cs only single pred}
set.seed(1111) #To keep reproducibility

outcomename = "island_side" #Name of outcome of interest

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #Sets K-fold cross-validation.

Npred <- ncol(data_train_cs_1)-1 #Sets predictors minus the outcome in column 1.

resultmat <- data.frame(Variable = names(data_train_cs_1)[-1], Accuracy = rep(0,Npred)) #Takes results and moves to dataframe named resultmat, measure set to accuracy.

for (n in 2:ncol(data_train_cs_1))
{
  fit22 <- caret::train( as.formula(paste(outcomename, "~",names(data_train_cs_1)[n])) , data = data_train_cs_1, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 10) 
resultmat[n-1,2]= max(fit22$results$Accuracy)  
} #Loop to all variables through prediction. Method rpart = tree algorithm, and skipping any NA values. 

print(resultmat)

```

```{r cs only multi}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit222 = caret::train(island_side  ~ ., data=data_train_cs_1, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 10) 

print(fit222$results)

prp(fit222$finalModel, extra = 8, type = 2)
```
```{r}
cs_not_best_tree <- prp(fit222$finalModel, extra = 8, type = 2, main = "Major Water Quality Predictors of Island Side")

ww=17.8/2.54; wh=ww; #for saving plot

dev.print(device=png,width=ww,height=wh,units="in",res=600,file="cs_not_best_tree.png") #save tree to file

file.move("./cs_not_best_tree.png", "../../results", overwrite = TRUE)
```

Well it appears that salinity prevails once again! The overwhelming success of salinity as a predictor suggests that the issue here is not with our predictors but actually with our outcome. Island side is too strongly influenced by salinity that our model cannot develop a method by which better accuracy is produced through the addition of further variables. This is very interesting information, however it does not address our overall objective to measure the efficacy of citizen science data collection. For this assessment, our alternate outcome of site type is more appropriate. 


We will end with a model evauation of site type. 

```{r cs only site type}

cs_only_site <- outcome_2 %>% dplyr::select(-Month, -Day, -Year, -military_time, -group_name)

cs_only_side <- na.omit(cs_only_site)

glimpse(cs_only_site)
```

```{r split data 4}
set.seed(123)

trainset <- caret::createDataPartition(y = cs_only_site$site_type, p = 0.7, list = FALSE)

#Extract observations/rows for training and assign to new variable
data_train_cs_2 = cs_only_site[trainset,] 

#Same as above but for the test set
data_test_cs_2 = cs_only_site[-trainset,] 
```

```{r null 4}
mlr::measureACC("Seagrass/Mangrove", cs_only_site$site_type)

```

```{r cs site type single}
set.seed(1111) #To keep reproducibility

outcomename = "site_type" #Name of outcome of interest

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #Sets K-fold cross-validation.

Npred <- ncol(data_train_cs_2)-1 #Sets predictors minus the outcome in column 1.

resultmat <- data.frame(Variable = names(data_train_cs_2)[-1], Accuracy = rep(0,Npred)) #Takes results and moves to dataframe named resultmat, measure set to accuracy.

for (n in 2:ncol(data_train_cs_2))
{
  fit33 <- caret::train( as.formula(paste(outcomename, "~",names(data_train_cs_2)[n])) , data = data_train_cs_2, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 10) 
resultmat[n-1,2]= max(fit33$results$Accuracy)  
} #Loop to all variables through prediction. Method rpart = tree algorithm, and skipping any NA values. 

print(resultmat)

```

```{r cs site type multi}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit333 = caret::train(site_type  ~ ., data=data_train_cs_2, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 10) 

print(fit333$results)

```
```{r}
cs_best_tree <- prp(fit333$finalModel, extra = 8, type = 2, main = "Major Water Quality Predictors of Site Type")
cs_best_tree

ww=17.8/2.54; wh=ww; #for saving plot

dev.print(device=png,width=ww,height=wh,units="in",res=600,file="cs_best_tree.png") #save tree to file

file.move("./cs_best_tree.png", "../../results", overwrite = TRUE)

```

Much better! Based on this tree it looks like we have identified the model that characterizes our outcome by a cascade of our water quality predictors. Consistent with our models above, salinity remains the primary branch seperating low salinity into Seagrass/Mangrove (likely all of our bayside sites) and high salinity into a mixture of all three site types. The second branch divdes along temperature lines which is once again consistent with the expected divisions as patch reefs tend to be cooler sites. The final split divides high saline and high temperature sites into coral reef and seagrass respectively. This division is based upon pH measures with greater pH representing Coral Reefs and lower pH representing Seagrass/Mangrove. This split is likely due to the average depth and proximity of Seagrass/Mangroves to shore and thus more strongly influenced by runoff/non point source pollution/ and rainfall. Though the accuracy of this model is similar to the accuracy of the model before removing all non-citizen science determined variables it is important to note that this model better represents the efficacy of citizen science data by eliminating confounding influences of data splitting.

Finally we will evaluate our test data with our model to produce a confusion matrix.

```{r}
train_predict <- predict(fit333, data_train_cs_2)
confusionMatrix(train_predict, data_train_cs_2$site_type)$table

test_predict <- predict(fit333, data_test_cs_2)
confusionMatrix(test_predict, data_test_cs_2$site_type)$table

```

Our model is not a very efficent at predicting the correct site type, however this is okay since the overall goal was not to make predictions but rather determine the major splitting points in the data. These major splitting points were serve as our measures of citizen science data collection, if they are consistent with the factors that typically classify our three site types of interest this will suggest that the data collected is indeed accurate. 






