---
title: "Citizen Science Efficacy Analysis"
author: "William Norfolk"
date: "11/6/2019"
output: word_document
---

This script measures the efficacy of citizen science data gathered in this study by modeling water quality parameters from data to predict the predict the resulting island side of predicted information. 

## Model Prep

Start by loading the libraries.
```{r add libs}
library('tidyr')
library('dplyr')
library('forcats')
library('ggplot2')
library('knitr')
library('caret')
library('doParallel')
library('rpart')
library('rpart.plot')
library('mda')
library('ranger')
library('e1071')
library('visdat')
```
Load the data and take a look.

```{r load and glimpse}
WQ_clean_data <- readRDS("../../data/processed_data/processeddata.rds")

glimpse(WQ_clean_data)
```

Check for NAs and properly formatted classes.

```{r visdat}
vis_dat(WQ_clean_data)
```
Data looks pretty good, there are some NA values but the tree models should be able to handel these since they are not strongly grouped. 

We do need to recode a few variables in this view. We will make all or date related variables numeric, and the remaining buckets will be factors. Also we can remove instructor_name and kit since they are not very relevent to the data. For now we will leave group_name in the data in case different schools are more effective measurers of water quality. 



```{r modify and vis}
#Drop instructor name and equipment
reduce_vars <- WQ_clean_data %>% dplyr::select(-instructor_name, -equipment)

#Change numerics
reduce_vars$Month <- as.numeric(as.character(reduce_vars$Month))
reduce_vars$Day <- as.numeric(as.character(reduce_vars$Day))
reduce_vars$Year <- as.numeric(as.character(reduce_vars$Year))

#Change factors
reduce_vars$group_name <- as.factor(as.character(reduce_vars$group_name))
reduce_vars$island_side <- as.factor(as.character(reduce_vars$island_side))
reduce_vars$site_type <- as.factor(as.character(reduce_vars$site_type))
reduce_vars$location <- as.factor(as.character(reduce_vars$location))


vis_dat(reduce_vars)
```

This data looks ready to use. We may make futher modifications downstream as models produce results.

## Data spllitting

Before we split we need ot ensure the NAs are removed from the outcome and the outcome is in the first column of the dataframe.

```{r drop NA outomes and reorder}
model_ready <- reduce_vars[!is.na(reduce_vars$island_side), ]

model_ready <- model_ready[, c(12,1,2,3,4,5,6,7,8,9,10,11,13)]

```


We will split our data for K-fold cross-validation into a 70/30 train and test set.


```{r data split}

set.seed(123)

trainset <- caret::createDataPartition(y = model_ready$island_side, p = 0.7, list = FALSE)

#Extract observations/rows for training and assign to new variable
data_train = model_ready[trainset,] 

#Same as above but for the test set
data_test = model_ready[-trainset,] 
```





```{r}
#For cores split if needed
```


## Null Model

First we need to determine our null model. Since our outcome is a categorical predictor, a null model should always predict the most common category. Four our dataset the oceanside is the most frequently visited island_side. We will use accuracy to measure how well our model predicts the island_side given the five water quality parameters.

```{r null model}

library(mlr) #We will jump between mlr and caret so call with :: at all times. 

mlr::measureACC("ocean", model_ready$island_side)

```

It looks like we have an accuracy of about 0.62 for our null lets see if we can do better. 


Now lets see if we can do better with some models. We will start with single a single predictor model to determine accuracy for each predictor variable.

```{r single-predictor tree acc}

set.seed(1111) #To keep reproducibility

outcomename = "island_side" #Name of outcome of interest

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) #Sets K-fold cross-validation.

Npred <- ncol(data_train)-1 #Sets predictors minus the outcome in column 1.

resultmat <- data.frame(Variable = names(data_train)[-1], Accuracy = rep(0,Npred)) #Takes results and moves to dataframe named resultmat, measure set to accuracy.

for (n in 2:ncol(data_train))
{
  fit1 <- caret::train( as.formula(paste(outcomename, "~",names(data_train)[n])) , data = data_train, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 10) 
resultmat[n-1,2]= max(fit1$results$Accuracy)  
} #Loop to all variables through prediction. Method rpart = tree algorithm, and skipping any NA values. 

print(resultmat)

```

So the single-predictor looks pretty interesting. The majority of the predictors are around the same level as the null, however we do have some spikes. Location and site_type have a very high accuracy. This is likely due to the fact that they are directly associated with the island_side. We will likely have to remove these down the line since they are too powerful. Salinity is a bit stronger than the null as well, this is likely due to the large split in salinity from each island side we saw in the visualization. This is likely to become a very strong predictor in the model, and there is a possibility that salinity alone is enough to establish island_side. 

We will continue with a full model.

```{r full-predictor tree acc}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit1 = caret::train(island_side  ~ ., data=data_train, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 10) 

print(fit1$results)

```
 Looks like we have pretty high accuracy across the board here. This is likely due to the two predictors mentioned above. Lets take a look at the tree.

```{r}
prp(fit1$finalModel, extra = 1, type = 1)
```

As expected our tree splits along the lines of site_type and location. This makes complete sense since island_side and site_type and essentially large subgroups of the various location information.To get a comprehensive picture of water qiality-based predictions we will remove these two variables from our data set. 

```{r reduce-further}

data_train <- data_train %>% dplyr::select(-location, -site_type)
data_test <- data_test %>% dplyr::select(-location, -site_type)
```

Lets try the models again
```{r single pred round two}
set.seed(1111) 
outcome = "season"
fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 
Npred <- ncol(data_train)-1
resultmat <- data.frame(Variable = names(data_train)[-1], Accuracy = rep(0,Npred))
for (n in 2:ncol(data_train))
{
  fit_single_reduced <- caret::train( as.formula(paste(outcomename, "~",names(data_train)[n])) , data = data_train, method = "rpart", trControl = fitControl, na.action = na.pass, tuneLength = 20) 
resultmat[n-1,2]= max(fit_single_reduced$results$Accuracy)  
}

print(resultmat)
```
This looks better, now for a full fit and tree.

```{r}
set.seed(1111)

fitControl <- trainControl(method="repeatedcv",number=5,repeats=5) 

fit2 = caret::train(island_side  ~ ., data=data_train, method="rpart",  trControl = fitControl, na.action = na.pass, tuneLength = 20) 

print(fit2$results)
```
With those two predictors removed accuracy is a bit higher than the null model (not quite as high as I had hoped). 

```{r}
prp(fit2$finalModel, extra = 1, type = 1)

```
The resulting tree (while a bit boring looking) is actually quite interesting. This suggests that salinity alone may be the best predictor of island_side.


Lets see if a random forest can break the data down further.

The random forest does not seem to handle NAs very well so we will remove all NAs from the data.

```{r}
data_train <- na.omit(data_train)
data_test <- na.omit(data_test)
```


```{r random-forest island_side}

set.seed(1111) #makes each code block reproducible
tuning_grid <- expand.grid( .mtry = seq(1,7,by=1), .splitrule = "gini", .min.node.size = seq(2,8,by=1) )
fit3 = caret::train(island_side ~ ., data=data_train, method="ranger",  trControl = fitControl, tuneGrid = tuning_grid, na.action = na.pass) 

```

```{r}
plot(fit3)
```

Based on the rando forest it looks like or data has the greatest accuracy when scaled with 6-7 predictors. 


